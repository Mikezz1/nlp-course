{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13qSDXPynRCR",
        "outputId": "fba96eda-4559-48dd-c07d-6f107b705422"
      },
      "outputs": [],
      "source": [
        "! pip -qq install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO24GvC3nmXf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import  AutoModelForQuestionAnswering, AutoTokenizer, AdamW, get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config():\n",
        "    num_epochs = 4\n",
        "    learning_rate = 4e-5\n",
        "    model_name = \"sberbank-ai/ruRoberta-large\"\n",
        "    batch_size = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gjmrRy6nrGC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! wget https://www.dropbox.com/sh/zhez7s5erqogogo/AAAf_lhCbjkMSA0hNEpQipj6a?dl=0 --content-disposition\n",
        "! unzip dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "687dLyqyn2_D"
      },
      "outputs": [],
      "source": [
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for passage in squad_dict['paragraphs']:\n",
        "        context = passage['context']\n",
        "        for qa in passage['qas']:\n",
        "            question = qa['question']\n",
        "            for answer in qa['answers']:\n",
        "                contexts.append(context)\n",
        "                questions.append(question)\n",
        "                answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "train_contexts, train_questions, train_answers = read_squad('sbersquad_train_clean_final.json')\n",
        "val_contexts, val_questions, val_answers = read_squad('sbersquad_dev_clean_final.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1paz5uYn3Hq"
      },
      "outputs": [],
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "  for answer, context in zip(answers, contexts):\n",
        "    gold_text = answer['text']\n",
        "    start_idx = answer['answer_start']\n",
        "    end_idx = start_idx + len(gold_text)\n",
        "    answer['answer_end'] = end_idx\n",
        "    \n",
        "    if context[start_idx:end_idx] == gold_text:\n",
        "        answer['answer_end'] = end_idx\n",
        "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "        answer['answer_start'] = start_idx - 1\n",
        "        answer['answer_end'] = end_idx - 1     \n",
        "    elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "        answer['answer_start'] = start_idx - 2\n",
        "        answer['answer_end'] = end_idx - 2     \n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B-8vBXsaogj"
      },
      "outputs": [],
      "source": [
        "model_name = Config.model_name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=Config.max_len)\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtwtpSKFaolU",
        "outputId": "7a98746d-7c85-42dc-8e25-1d4143a4ea55"
      },
      "outputs": [],
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = Config.max_len-1\n",
        "          \n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'])\n",
        "            \n",
        "            if end_positions[-1] is None:\n",
        "              count += 1\n",
        "              end_positions[-1] = Config.max_len-1\n",
        "\n",
        "    print(count)\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xBQMcyXa-yA"
      },
      "outputs": [],
      "source": [
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k4jJy4x_I0T"
      },
      "outputs": [],
      "source": [
        "def loss_fn(preds, labels):\n",
        "    start_preds, end_preds = preds\n",
        "    start_labels, end_labels = labels\n",
        "    \n",
        "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
        "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
        "    total_loss = (start_loss + end_loss) / 2\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcTphSjia-2n"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name) \n",
        "model.to(device)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNGwA6kEfHD1",
        "outputId": "4e2a9f0a-e8a9-4d1e-ccd4-cb77275fab2c"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ax1JyEHnx6H"
      },
      "outputs": [],
      "source": [
        "def validate(valid_loader, model):\n",
        "    model.eval()\n",
        "    loss_hist = 0\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()  \n",
        "    for batch in tqdm(valid_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        labels = torch.stack((start_positions, end_positions))\n",
        "        out = model(input_ids, attention_mask=attention_mask)\n",
        "        start, end = out['start_logits'], out['end_logits']\n",
        "        loss = loss_fn(torch.stack((start, end)), labels).detach().to('cpu')\n",
        "\n",
        "        loss_hist += loss\n",
        "    print('...................')\n",
        "    print(f'Validation loss: {loss_hist/len(valid_loader)}')\n",
        "\n",
        "\n",
        "def train_epoch(train_loader, optim, model, scheduler):\n",
        "    print(\"Epoch %s of %s\" %(epoch + 1, Config.num_epochs))\n",
        "    epoch_loss = 0\n",
        "    i=0\n",
        "    for batch in tqdm(train_loader):\n",
        "        gc.collect()\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.detach().to('cpu')\n",
        "\n",
        "        if i % 300 == 0:\n",
        "            print(f'Step: {i}, loss: {loss}')\n",
        "        i+=1\n",
        "    print('.......................................')\n",
        "    print(f\"Loss: {epoch_loss / len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "mIsFFyoOykZb",
        "outputId": "aee10fc8-cfb0-45f1-908f-987461235ca9"
      },
      "outputs": [],
      "source": [
        "optim = AdamW(model.parameters(), lr=Config.learning_rate)\n",
        "\n",
        "max_train_steps = len(train_loader)*Config.num_epochs\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "            optim,\n",
        "            num_warmup_steps=max_train_steps // 50,\n",
        "            num_training_steps=max_train_steps\n",
        "            )\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(Config.num_epoch):\n",
        "    print(f'====== EPOCH #{epoch} ======')\n",
        "    train_epoch(train_loader, optim, model, scheduler)\n",
        "    validate(val_loader, optim, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2iRQonzlHO0"
      },
      "outputs": [],
      "source": [
        "# load test set\n",
        "test_contexts, test_questions, test_answers = read_squad('sbersquad_test_data_to_solve.json')\n",
        "add_end_idx(test_answers, test_contexts)\n",
        "test_encodings = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "test_dataset = SquadDataset(test_encodings)\n",
        "test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqYn8fw6l_vm",
        "outputId": "54a8690a-5307-4930-ffe9-8c16be388603"
      },
      "outputs": [],
      "source": [
        "# get answers on test set\n",
        "model.eval()\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "start_ids = []\n",
        "end_ids = []\n",
        "for batch in tqdm(test_loader):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    start_ids.append(torch.softmax(outputs[0], dim = 1).detach().to('cpu').numpy())\n",
        "    end_ids.append(torch.softmax(outputs[1], dim = 1).detach().to('cpu').numpy())\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNNYBA4Hrebs",
        "outputId": "c8f85c55-e202-4bbc-9129-1bc1b9e665bb"
      },
      "outputs": [],
      "source": [
        "# post-process answers\n",
        "start = np.concatenate(start_ids, axis = 0)\n",
        "end = np.concatenate(end_ids, axis = 0)\n",
        "print(start.shape)\n",
        "\n",
        "answer_starts = np.argmax(start, axis=1)  \n",
        "answer_ends = np.argmax(end, axis=1) + 1  \n",
        "\n",
        "answers = [tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(test_encodings['input_ids'][i][answer_start:answer_end])) \\\n",
        "          for i, answer_start, answer_end in zip(list(range(len(test_encodings['input_ids']))), answer_starts, answer_ends)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2_6ksEl1RJS"
      },
      "outputs": [],
      "source": [
        "# create submission\n",
        "def process_json(df, answers, ansers_start):\n",
        "  l= []\n",
        "  k = 0\n",
        "  for i in range(len(df)):\n",
        "    for qst in range(len(df['paragraphs'][i]['qas'])):\n",
        "      for answer in range(len(df['paragraphs'][i]['qas'][qst]['answers'])):\n",
        "        df['paragraphs'][i]['qas'][qst]['answers'][answer]['answer_start'] = int(answer_starts[k])\n",
        "        df['paragraphs'][i]['qas'][qst]['answers'][answer]['text'] = answers[k]\n",
        "        k+=1\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "test_df = pd.read_json(\"sbersquad_test_data_to_solve.json\")\n",
        "new_df = process_json(test_df, answers, answer_starts)\n",
        "sub = {}\n",
        "sub['title'] = list(new_df['title'].values)\n",
        "sub['paragraphs'] = list(new_df['paragraphs'].values)\n",
        "with open('submission.json', 'w') as f:\n",
        "    json.dump(sub, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7opocDKVXnS",
        "outputId": "bbf9e176-af37-4d30-90fd-16f8c9a339ea"
      },
      "outputs": [],
      "source": [
        "! zip submission.zip submission.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "custom_squad_hw3_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
