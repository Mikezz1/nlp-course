{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "182ae95f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "182ae95f",
        "outputId": "fd7865a4-b5e9-4e52-d90b-7233f0cab802"
      },
      "outputs": [],
      "source": [
        "# ! pip install -qq transformers \n",
        "# ! pip install -qq fasttext \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import nltk.data \n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from torch.optim import SGD, Adam\n",
        "from warnings import filterwarnings\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "import fasttext \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "filterwarnings('ignore')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "521df96e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "seed_everything(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f24a02e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class cfg():\n",
        "    data_dir =  './data/'\n",
        "    embedding_dim = 200\n",
        "    n_filtters = 100\n",
        "    filter_sizes = [2,4,7]\n",
        "    output_dim = 1\n",
        "    dropout_prob = 0.3\n",
        "    num_epochs = 5\n",
        "    batch_size = 32\n",
        "    downsample = True\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3801c00",
      "metadata": {
        "id": "e3801c00"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_df = pd.read_csv(cfg.data_dir + 'train.tsv', sep='\\t')\n",
        "test_df = pd.read_csv(cfg.data_dir + 'test.tsv', sep='\\t')\n",
        "valid_df = pd.read_csv(cfg.data_dir + 'valid.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a05e1e",
      "metadata": {},
      "source": [
        "### Preprocessing\n",
        "Props to: <br>\n",
        "https://github.com/akutuzov/webvectors/blob/master/preprocessing/modular_processing/unify.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472bff7c",
      "metadata": {
        "id": "472bff7c"
      },
      "outputs": [],
      "source": [
        "def list_replace(search, replacement, text):\n",
        "    '''\n",
        "    Replaces all symbols of text which are present\n",
        "    in the search string with the replacement string.\n",
        "    '''\n",
        "    search = [el for el in search if el in text]\n",
        "    for c in search:\n",
        "        text = text.replace(c, replacement)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = list_replace(\n",
        "        '\\u00AB\\u00BB\\u2039\\u203A\\u201E\\u201A\\u201C\\u201F\\u2018\\u201B\\u201D\\u2019',\n",
        "         '\\u0022',\n",
        "          text\n",
        "    )\n",
        "\n",
        "    text = list_replace(\n",
        "        '\\u2012\\u2013\\u2014\\u2015\\u203E\\u0305\\u00AF',\n",
        "         '\\u2003\\u002D\\u002D\\u2003',\n",
        "          text\n",
        "    )\n",
        "\n",
        "    text = list_replace(\n",
        "        '\\u2010\\u2011',\n",
        "         '\\u002D',\n",
        "          text\n",
        "    )\n",
        "\n",
        "    text = list_replace(\n",
        "        '\\u2000\\u2001\\u2002\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200A\\u200B\\u202F\\u205F\\u2060\\u3000',\n",
        "        '\\u2002',\n",
        "        text\n",
        "    )\n",
        "\n",
        "    text = re.sub('\\u2003\\u2003', '\\u2003', text)\n",
        "    text = re.sub('\\t\\t', '\\t', text)\n",
        "\n",
        "    text = list_replace(\n",
        "        '\\u02CC\\u0307\\u0323\\u2022\\u2023\\u2043\\u204C\\u204D\\u2219\\u25E6\\u00B7\\u00D7\\u22C5\\u2219\\u2062',\n",
        "        '.',\n",
        "         text\n",
        "    )\n",
        "\n",
        "    text = list_replace('\\u2217', '\\u002A', text)\n",
        "\n",
        "    text = list_replace('…', '...', text)\n",
        "\n",
        "    text = list_replace('\\u00C4', 'A', text)\n",
        "    text = list_replace('\\u00E4', 'a', text)\n",
        "    text = list_replace('\\u00CB', 'E', text)\n",
        "    text = list_replace('\\u00EB', 'e', text)\n",
        "    text = list_replace('\\u1E26', 'H', text)\n",
        "    text = list_replace('\\u1E27', 'h', text)\n",
        "    text = list_replace('\\u00CF', 'I', text)\n",
        "    text = list_replace('\\u00EF', 'i', text)\n",
        "    text = list_replace('\\u00D6', 'O', text)\n",
        "    text = list_replace('\\u00F6', 'o', text)\n",
        "    text = list_replace('\\u00DC', 'U', text)\n",
        "    text = list_replace('\\u00FC', 'u', text)\n",
        "    text = list_replace('\\u0178', 'Y', text)\n",
        "    text = list_replace('\\u00FF', 'y', text)\n",
        "    text = list_replace('\\u00DF', 's', text)\n",
        "    text = list_replace('\\u1E9E', 'S', text)\n",
        "    # Removing punctuation\n",
        "    text = list_replace(',.[]{}()=+-−*&^%$#@!~;:§/\\|\\?\\'\\n', ' ', text)\n",
        "    # Replacing all numbers with masks\n",
        "    text = list_replace('0123456789', 'x', text)\n",
        "\n",
        "    currencies = list(\n",
        "            '\\u20BD\\u0024\\u00A3\\u20A4\\u20AC\\u20AA\\u2133\\u20BE\\u00A2\\u058F\\u0BF9\\u20BC\\u20A1\\u20A0\\u20B4\\u20A7\\u20B0\\u20BF\\u20A3\\u060B\\u0E3F\\u20A9\\u20B4\\u20B2\\u0192\\u20AB\\u00A5\\u20AD\\u20A1\\u20BA\\u20A6\\u20B1\\uFDFC\\u17DB\\u20B9\\u20A8\\u20B5\\u09F3\\u20B8\\u20AE\\u0192'\n",
        "    )\n",
        "\n",
        "    alphabet = list(\n",
        "        '\\t\\r абвгдеёзжийклмнопрстуфхцчшщьыъэюяАБВГДЕЁЗЖИЙКЛМНОПРСТУФХЦЧШЩЬЫЪЭЮЯabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ '\n",
        "    )\n",
        "\n",
        "    allowed = set(currencies + alphabet)\n",
        "\n",
        "    cleaned_text = [sym for sym in text if sym in allowed]\n",
        "    cleaned_text = ''.join(cleaned_text)\n",
        "\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15b5e89e",
      "metadata": {
        "id": "15b5e89e"
      },
      "outputs": [],
      "source": [
        "# Extracting tweet labels\n",
        "train_labels = train_df['label'].values\n",
        "valid_labels = valid_df['label'].values\n",
        "\n",
        "def process(df):\n",
        "    tweets = df.tweet.values\n",
        "    return \" \".join([clean_text(tweet).lower() for tweet in tweets])\n",
        "\n",
        "train_df['clean_text'] = process(train_df)\n",
        "valid_df['clean_text'] = process(valid_df)\n",
        "test_df['clean_text'] = process(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16dc4781",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_positive_class_df = train_df[train_df['label'] == 1]\n",
        "train_negative_class_df = train_df[train_df['label'] == 0]\n",
        "\n",
        "num_positive_examples = len(train_positive_class_df)\n",
        "num_negative_examples = len(train_negative_class_df)\n",
        "\n",
        "if cfg.downsample:\n",
        "    train_positive_class_df = train_positive_class_df.sample(num_negative_examples,\n",
        "                                                            replace=True)\n",
        "    train_df = pd.concat((train_positive_class_df, train_negative_class_df)).sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6vO-GDRP5wEO",
      "metadata": {
        "id": "6vO-GDRP5wEO"
      },
      "outputs": [],
      "source": [
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "def preproc(text):\n",
        "    text = tokenizer.tokenize(text)\n",
        "    return [word for word in text if word not in stopwords.words('russian') + [' ', '\\n']]\n",
        "    \n",
        "def pad_to_max_len(tweet, max_len):\n",
        "    tweet_len = len(tweet)\n",
        "    padding = [\"<PAD>\"]*(max_len - tweet_len)\n",
        "    tweet += padding\n",
        "    return tweet\n",
        "\n",
        "def get_padded_data(texts, vocab, max_len=70):\n",
        "\n",
        "    word2idx = vocab.word_to_idx\n",
        "    pad = pad_sequence([torch.as_tensor([word2idx[w] if w in vocab.vocab else word2idx['<UNK>']\n",
        "                                                   for w in seq][:max_len]) for seq in texts], \n",
        "                               batch_first=True)\n",
        "\n",
        "    return pad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946f6235",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "946f6235",
        "outputId": "cf42a5ae-b597-44ca-ace4-3d76d8e2d5de"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Vocab():\n",
        "        def __init__(self, df):\n",
        "            self.tweets = df.clean_text.values\n",
        "            self.vocab = {}\n",
        "            self.vocab_size = 0\n",
        "            self.idx_to_word = {}\n",
        "            self.word_to_idx = {}\n",
        "    \n",
        "        def build_vocab(self, max_size = None):\n",
        "            all_text = preproc(\" \".join(self.tweets))\n",
        "            self.vocab = set(all_text)\n",
        "            self.vocab_size = len(self.vocab)\n",
        "            counter = {word: 1 for word in self.vocab}\n",
        "            for word in all_text:\n",
        "                counter[word] += 1\n",
        "            freqs = {word: freq for word, freq in sorted(counter.items(), key=lambda freq: -freq[1])}\n",
        "            \n",
        "            # limit max_size\n",
        "            if max_size:\n",
        "                new_vocab = {}\n",
        "                for i, word in enumerate(freqs):\n",
        "                    new_vocab.add(word)\n",
        "                    if i == max_size-1:\n",
        "                        break\n",
        "                        \n",
        "                self.vocab = new_vocab\n",
        "                    \n",
        "            self.idx_to_word = {idx: word for idx,word in enumerate(self.vocab,1)}\n",
        "            self.idx_to_word[0] = '<UNK>'\n",
        "            self.word_to_idx = {word: idx for idx,word in enumerate(self.vocab,1)}\n",
        "            self.word_to_idx['<UNK>'] = 0\n",
        "    \n",
        "        \n",
        "vocabulary = Vocab(train_df)\n",
        "vocabulary.build_vocab()\n",
        "        \n",
        "\n",
        "class TwitterDataset(Dataset):\n",
        "        def __init__(self, tweets, targets, vocab, tokenizer, max_len):\n",
        "            self.targets = targets\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_len = max_len\n",
        "            self.vocab = vocab\n",
        "            self.tweets =  get_padded_data(\n",
        "                    tweets.apply(preproc),\n",
        "                    self.vocab,\n",
        "                    self.max_len\n",
        "                )\n",
        "\n",
        "        def __len__(self):\n",
        "              return len(self.tweets)\n",
        "\n",
        "        def __getitem__(self, item):\n",
        "            text = self.tweets[item]\n",
        "            target = self.targets[item]\n",
        "            return text, torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "    \n",
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "train_tokenized = [tokenizer.tokenize(x) for x in train_df.tweet]\n",
        "valid_tokenized = [tokenizer.tokenize(x) for x in valid_df.tweet]\n",
        "test_tokenized = [tokenizer.tokenize(x) for x in test_df.tweet]\n",
        "\n",
        "train_max_len = max(map(len, train_tokenized))\n",
        "valid_max_len = max(map(len, valid_tokenized))\n",
        "test_max_len = max(map(len, valid_tokenized))\n",
        "\n",
        "print(train_max_len)\n",
        "print(valid_max_len)\n",
        "print(test_max_len)\n",
        "\n",
        "    \n",
        "def create_data_loader(df, vocab, tokenizer, batch_size, max_len, target=True):\n",
        "        if 'label' in df:\n",
        "            labels = df.label.values\n",
        "        else:\n",
        "            labels = [0] * len(df)\n",
        "        ds = TwitterDataset(\n",
        "            tweets= df.clean_text,\n",
        "            targets=labels,\n",
        "            vocab = vocab,\n",
        "            tokenizer=tokenizer,\n",
        "            max_len=max_len,\n",
        "        )\n",
        "        return DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gMHnFJ44GZtC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMHnFJ44GZtC",
        "outputId": "85558502-09c4-4edd-8e41-821c96a9cc35"
      },
      "outputs": [],
      "source": [
        "fasttext_model = fasttext.load_model('rudrec_fasttext_model.bin')\n",
        "embedding_matrix = np.zeros((vocabulary.vocab_size + 2, 200))\n",
        "\n",
        "for i, word in enumerate(vocabulary.word_to_idx,1):\n",
        "    embedding_vector = fasttext_model.get_word_vector((word))\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_matrix = torch.Tensor(embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e676ca90",
      "metadata": {
        "id": "e676ca90"
      },
      "outputs": [],
      "source": [
        "class CNN_classifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters,filter_sizes, output_dim, dropout_prob):\n",
        "        super(CNN_classifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        return self.fc(cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = cfg.batch_size\n",
        "train_dataloader = create_data_loader(train_df, vocabulary, tokenizer, BATCH_SIZE, train_max_len)\n",
        "valid_dataloader = create_data_loader(valid_df, vocabulary, tokenizer, BATCH_SIZE, valid_max_len)\n",
        "test_datloader = create_data_loader(test_df, vocabulary, tokenizer, BATCH_SIZE, test_max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "763b2ccb",
      "metadata": {
        "id": "763b2ccb"
      },
      "outputs": [],
      "source": [
        "vocab_size = vocabulary.vocab_size + 1\n",
        "device = 'cpu'\n",
        "model = CNN_classifier(vocab_size,\n",
        "                      cfg.embedding_dim,\n",
        "                      cfg.n_filters,\n",
        "                      cfg.filter_sizes,\n",
        "                      cfg.output_dim,\n",
        "                      cfg.dropout_prob\n",
        "                      )\n",
        "model.to(device)\n",
        "\n",
        "N_EPOCHS = cfg.num_epochs\n",
        "\n",
        "optimizer = Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k_Zlty495wET",
      "metadata": {
        "id": "k_Zlty495wET"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(F.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    \n",
        "    acc_hist =  []\n",
        "    loss_hist = []\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    for text,label in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text.to(device)).squeeze(1)\n",
        "        loss = criterion(pred.float(), label.float().to(device))\n",
        "\n",
        "        acc = binary_accuracy(pred.float(), label.float().to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss\n",
        "        epoch_acc += acc\n",
        "        acc_hist.append(acc)\n",
        "        loss_hist.append(loss)\n",
        "        \n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n",
        "        \n",
        "        \n",
        "def evaluate(model, eval_dataloader, criterion):\n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for text,label in eval_dataloader:\n",
        "            pred = model(text.to(device)).squeeze(1)\n",
        "            loss = criterion(pred.float(), label.float().to(device))\n",
        "            acc = binary_accuracy(pred.float(), label.float().to(device))\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(eval_dataloader), epoch_acc / len(eval_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c85c08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "c0c85c08",
        "outputId": "a3232648-a0b8-4697-ac0e-e7b419a42f38"
      },
      "outputs": [],
      "source": [
        "for epoch in tqdm(range(N_EPOCHS)):\n",
        "    train_loss, train_acc = train_epoch(model, train_dataloader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_dataloader, criterion)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%,\\\n",
        "    Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71bcecc8",
      "metadata": {
        "id": "71bcecc8"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "for batch in test_datloader:\n",
        "    text, _ = batch\n",
        "    text = text.to(device)\n",
        "    predictions.append(model(text))\n",
        "    \n",
        "predictions = torch.sigmoid(torch.cat(predictions)).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad79f9ac",
      "metadata": {
        "id": "ad79f9ac"
      },
      "outputs": [],
      "source": [
        "df_submit = pd.DataFrame(columns=['tweet_id', 'label'])\n",
        "\n",
        "df_submit['tweet_id'] = test_df['tweet_id'].values\n",
        "df_submit['label'] = predictions\n",
        "\n",
        "df_submit.to_csv('solution.csv', sep=',', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rnn_cnn_v2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "161c945532039f7d57aa33e4253d854ac273b60397325c019a7bd57c2c7e143b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
